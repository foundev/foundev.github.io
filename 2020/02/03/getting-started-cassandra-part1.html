---
layout: post
title: Starting Cassandra - Setting up a Multi-DC environment
tags: [cassandra]
---
<p>This is a quick and dirty opinionated guide to setting up a Cassandra cluster with multiple data centers.</p>
<h2>A new cluster</h2>
<ul>
<li>In cassandra.yaml set <code>endpoint_snitch: GossipingPropertyFileSnitch</code>, some prefer PropertyFileSnitch for the ease of pushing out one file. GossipingPropertyFileSnitch is harder to get wrong in my experience.</li>
<li>set dc in cassandra-rackdc.properties. Set to be whatever dc you want that node to be in. Ignore rack until you really need it, 8/10 people that use racks do it wrong the first time, and it's slightly painful to unwind.</li>
<li>finish adding all of your nodes.</li>
<li>if using authentication,  set <code>system_auth</code> keyspace to use NetworkTopologyStrategy in cqlsh with RF 3 (or == number of replicas if less than 3 per dc) for each datacenter you've created <code>ALTER KEYSPACE system_auth WITH REPLICATION= {'class' : 'NetworkTopologyStrategy', 'data_center_name' : 3, 'data_center_name' : 3};</code>, run repair after changing RF</li>
<li><code>nodetool repair -pr system_auth</code> on each node in the cluster on the new keyspace.</li>
<li>create your new keyspaces for your app with RF 3 in each dc (much like you did for the <code>system_auth</code> step above).</li>
<li><code>nodetool repair -pr whatever_new_keyspace</code> on each node in the cluster on the new keyspace.</li>
</ul>
<h2>An existing cluster</h2>
<p>This is harder and involves more work and more options, but I'm going to discuss the way that gets you into the least amount of trouble operationally.</p>
<ul>
<li>make sure <em>none</em> of the drivers you use to connect to Cassandra are using DowngradingConsistencyRetryPolicy, or using the maligned withUsedHostsPerRemoteDc, especially allowRemoteDCsForLocalConsistencyLevel, as this may cause your driver to send requests to the remote data center before it's populated with data.</li>
<li>switch <code>endpoint_snitch</code> on each node to GossipingPropertyFileSnitch</li>
<li>set dc in cassandra-rackdc.properties. Set to be whatever dc you want that node to be in. Ignore rack until you really need it, 8/10 people that use racks do it wrong the first time, and it's slightly painful to unwind.</li>
<li>bootstrap each node in the new data center.</li>
<li>if using authentication,  set <code>system_auth</code> keyspace to use NetworkTopologyStrategy in cqlsh with RF 3 (or == number of replicas if less than 3 per dc) for each datacenter you've created <code>ALTER KEYSPACE system_auth WITH REPLICATION= {'class' : 'NetworkTopologyStrategy', 'data_center_name' : 3, 'data_center_name' : 3};</code>, run repair after changing RF</li>
<li><code>nodetool repair -pr system_auth</code> on each node in the cluster on the new keyspace.</li>
<li>alter your app keyspaces for your app with RF 3 in each dc (much like you did for the <code>system_auth</code> step above), </li>
<li><code>nodetool repair -pr whatever_keyspace</code> on each node in the cluster on the new keyspace.</li>
</ul>
<p>enjoy new data center</p>
<h3>how to get data to new dc</h3>
<h4>Repair approach</h4>
<p>Best done with if your repair jobs can't be missed or stopped, either because you have a process like OpsCenter or Reaper running repairs. It also has the advantage of being very easy, and if you've already automated repair you're basically done.</p>
<ul>
<li>let repair jobs continue..that's it!</li>
</ul>
<h4>Rebuild approach</h4>
<p>Faster less resource intensive, and if you have enough time to complete it while repair is stopped. Rebuild is easier to 'resume' than repair in many ways, so this has a number of advantages.</p>
<ul>
<li>run <code>nodetool rebuild</code> on each node in the new dc only, if it dies for some reason, rerunning the command will resume the process.</li>
<li>run <code>nodetool cleanup</code></li>
</ul>
<h4>YOLO rebuild with repair</h4>
<p>This will probably over stream it's share of data and honestly a lot of folks do this for some reason in practice:</p>
<ul>
<li>leave repair jobs running</li>
<li>run <code>nodetool rebuild</code> on each node in the new dc only, if it dies for some reason, rerunning the command will resume the process.</li>
<li>run <code>nodetool cleanup</code> on each node</li>
</ul>
<h2>Cloud strategies</h2>
<p>There are a few valid approaches to this and none of them are wrong IMO.</p>
<h3>region == DC, rack == AZ</h3>
<p>Will need to get into racks and a lot of people get this wrong and imbalance the racks, but you get the advantage of more intelligent failure modes, with racks mapping to AZs.</p>
<h3>AZ..regardless of region == DC</h3>
<p>This allows things to be balanced easily, but you have no good option for racks then. However, some people think racks are overrated, and I'd say a majority of clusters run with one rack.</p>