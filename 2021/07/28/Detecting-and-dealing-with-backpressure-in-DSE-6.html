---
title: Detecting and dealing with backpressure in DSE 6.x
date: 2021-07-28T08:11:00+00:00
author: Ryan Svihla
layout: post
tags: [ dse, backpressure, DataStax ]
---
<p>Several years ago DataStax change DataStax Enterprise from a version of Cassandra with some closed source plugins and a handful of fixes most of which were backports of
fixes in very old versions, to <a href="https://www.datastax.com/blog/dse-advanced-performance-apache-cassandratm-goes-turbo">a fundamentally different architecture</a>. This new architecure we will
refer to as TPC, however, there were also several internals that were changed that may have relied or depended on TPC but were not actually themselves part of TPC.</p>
<p>Today I want to review one of those features TPC Backpressure. TPC Backpressure in theory stops the node from running out of memory or becoming unresponsive and being somewhat down (in Cassandra this is often worse than being down). TPC Backpressure
has a complex implementation and can be difficult to explain, but for the sake of brevity I am going to focus on two primary thresholds that when passed the node will
pause new requests allowing the existing requests to complete.</p>
<h2>log messages</h2>
<p>backpressure active regex, tells you when it is engaged</p>
<p><code>shell
"DEBUG\s+\[(?P&lt;thread&gt;.*)\] (?P&lt;date&gt;.{10} .{12}) *(?P&lt;source_file&gt;[^:]*):(?P&lt;source_line&gt;[0-9]*) - TPC backpressure is active on core (?P&lt;core_num&gt;\d+) with global local/remote pending tasks at (?P&lt;global_pending&gt;\d+)/(?P&lt;remote_pending&gt;\d+)"</code></p>
<p>You can also just use <a href="https://github.com/BurntSushi/ripgrep#installation">ripgrep</a> for <code>TPC backpressure is active</code> and see how often that is enabled. If I want to be extra fancy I can take the regex above, combine it with regrep and output the remoting and global pending values:</p>
<p>for global:</p>
<p><code>rg "DEBUG\s+\[(?P&lt;thread&gt;.*)\] (?P&lt;date&gt;.{10} .{12}) *(?P&lt;source_file&gt;[^:]*):(?P&lt;source_line&gt;[0-9]*) - TPC backpressure is active on core (?P&lt;core_num&gt;\d+) with global local/remote pending tasks at (?P&lt;global_pending&gt;\d+)/(?P&lt;remote_pending&gt;\d+)" -or  '$global_pending'</code></p>
<p>for remote:</p>
<p><code>rg "DEBUG\s+\[(?P&lt;thread&gt;.*)\] (?P&lt;date&gt;.{10} .{12}) *(?P&lt;source_file&gt;[^:]*):(?P&lt;source_line&gt;[0-9]*) - TPC backpressure is active on core (?P&lt;core_num&gt;\d+) with global local/remote pending tasks at (?P&lt;global_pending&gt;\d+)/(?P&lt;remote_pending&gt;\d+)" -or  '$remote_pending'</code></p>
<p>The higher it is the longer it will take to clear out, this can be used to measure severity, once you have the total you can try and find the time it occured at. You can find the 5 worse totals with the following</p>
<p><code>sh
rg "DEBUG\s+\[(?P&lt;thread&gt;.*)\] (?P&lt;date&gt;.{10} .{12}) *(?P&lt;source_file&gt;[^:]*):(?P&lt;source_line&gt;[0-9]*) - TPC backpressure is active on core (?P&lt;core_num&gt;\d+) with global local/remote pending tasks at (?P&lt;global_pending&gt;\d+)/(?P&lt;remote_pending&gt;\d+)"  -or  '$global_pending' --no-filename | sort -n | tail -n 5
134
813
1111
1831
1911</code></p>
<p>do your grep again but filter out for the worst value</p>
<p><code>sh
rg "DEBUG\s+\[(?P&lt;thread&gt;.*)\] (?P&lt;date&gt;.{10} .{12}) *(?P&lt;source_file&gt;[^:]*):(?P&lt;source_line&gt;[0-9]*) - TPC backpressure is active on core (?P&lt;core_num&gt;\d+) with global local/remote pending tasks at (?P&lt;global_pending&gt;\d+)/(?P&lt;remote_pending&gt;\d+)" | rg 1911
debug.log:DEBUG [CoreThread-1] 2020-01-01 01:01:01,000  NoSpamLogger.java:92 - TPC backpressure is active on core 1 with global local/remote pending tasks at 1911/120</code></p>
<p>Now you can compare that to node latencies at that time and see if that matches the problems you are trying to find with latency and timesouts.</p>
<h2>what to do</h2>
<p>There is some art to the next step, one needs to evaluate if raising backpressure will actually help or hurt the situation. But let us say you see the GC is low, the CPU usage is low
and the latencies are bad, it just could be a workload that the defaults are too low.</p>
<h3>checking existing state</h3>
<p>To know the next steps we need to take state of the system</p>
<h4>Check GC</h4>
<p><code>sperf core gc</code> is a good tool for this, and can quickly tell you how severe gc is. For example in this report:</p>
<p>```sh
gcinspector version 0.6.4</p>
<h2>. &lt;300ms + 301-500ms ! &gt;500ms</h2>
<p>2020-01-10 16:28:37 236 ...............+++........++.....+.+...+++.++.+.+......+.....+.!!+!.!++!+..++.+.!..+.!++++.!.++++.+.+++..+!!.......!..+.+....+.....++.+...+.+.++.+.+!!++.+.+.!++.+.+...+.+..+.+.+.+..+++....++..+++....+..++.+++.+!+..+.+.+.+!......+++....+</p>
<p>busiest period: 2020-01-10 16:28:37 (75041ms)</p>
<p>GC pauses  max        p99        p75        p50        p25        min      <br />
           ---        ---        ---        ---        ---        ---      <br />
ms         800        729        358        282        243        201        </p>
<p>Worst pauses in ms:
[800, 735, 729]</p>
<h2>Collections by type</h2>
<ul>
<li>G1 Young: 236
```</li>
</ul>
<p>The GC is extremely heavy and the target gc rate was 500ms, this is not a cluster you wan to raise backpressure on. Now let us look at a cluster that has good GC and would be at least
safe to raise backpressure limits on:</p>
<p>```sh
gcinspector version 0.6.4</p>
<h2>. &lt;300ms + 301-500ms ! &gt;500ms</h2>
<p>2020-01-10 16:28:37 10 .........</p>
<p>busiest period: 2020-01-10 16:28:37 (75041ms)</p>
<p>GC pauses  max        p99        p75        p50        p25        min      <br />
           ---        ---        ---        ---        ---        ---      <br />
ms         300        289        250        225        210        201        </p>
<p>Worst pauses in ms:
[300, 289, 250]</p>
<h2>Collections by type</h2>
<ul>
<li>G1 Young: 10
```</li>
</ul>
<p>In this case the system is not very busy with GC activity and if there are high pending writes and reads, and we find backpressure is being enabled, raising those limits may help system performance.</p>
<h4>Check Pending</h4>
<p><code>sperf core statuslogger</code> can generate some very useful reports to track the number of pending operations in your cluster. Below I've shared an artificial example:</p>
<p>```
sperf core statuslogger version: 0.6.13</p>
<p>Summary (111,111 lines)
Summary (0 skipped lines)</p>
<p>dse versions: {'6.x'}
cassandra versions: unknown
first log time: 2021-01-01 01:11:01.111000+00:00
last log time: 2021-01-01 01:01:01.111000+00:00
duration: 10.00 minutes
total stages analyzed: 111
total nodes analyzed: 1</p>
<p>GC pauses  max        p99        p75        p50        p25        min
           ---        ---        ---        ---        ---        ---
ms         500       500         400        350        250        250
total GC events: 11</p>
<h2>busiest tables by ops across all nodes</h2>
<ul>
<li>debug.log: test.tester: 1,000,000 ops / 11.11 mb data</li>
</ul>
<h2>busiest table by data across all nodes</h2>
<ul>
<li>debug.log: test.tester: 111,111 ops / 11.11 mb data</li>
</ul>
<h2>busiest stages across all nodes</h2>
<ul>
<li>TPC/9 local backpressure:                         11115  (debug.log)</li>
<li>TPC/4 local backpressure:                         6791   (debug.log)</li>
<li>TPC/all/BACKPRESSURED_MESSAGE_DECODE active:      6618   (debug.log)</li>
<li>TPC/4/BACKPRESSURED_MESSAGE_DECODE active:        5743   (debug.log)</li>
<li>TPC/1 local backpressure:                         5414   (debug.log)</li>
<li>TPC/2 local backpressure:                        5111   (debug.log)</li>
<li>TPC/all/WRITE_REMOTE pending:                     4967   (debug.log)</li>
<li>TPC/all/WRITE_RESPONSE active:                    4771   (debug.log)</li>
<li>TPC/3/WRITE_RESPONSE active:                      4460   (debug.log)</li>
</ul>
<h2>busiest stages in PENDING</h2>
<p>debug.log:
       TPC/all/WRITE_REMOTE:             4967
       TPC/all/READ_LOCAL:               4599
       TPC/3/WRITE_REMOTE:               1455
       TPC/1/WRITE_REMOTE:               1442
       TPC/2/READ_LOCAL:                 1441
       TPC/3/READ_LOCAL:                 1034
       TPC/9/READ_LOCAL:                 935
       TPC/9/WRITE_REMOTE:               988</p>
<h2>busiest LOCAL BACKPRESSURE</h2>
<p>debug.log:
       avg backpressure                  1332.06
       max backpressure                  11111
```</p>
<p>Here we can see there is a lot of backpressure and realitively little GC happening in this case this would suggest to me we can raise the backpressure limits to see if latency improves and more of the system is utilized.</p>
<h3>raising backpressure limits</h3>
<p>The two main paramters I like to focus on are (both in the cassandra.yaml):</p>
<p><code>tpc_concurrent_requests_limit</code> -  the number of concurrently executed read/write requests per core, to be increased only if the CPU is under utilized, or decreased only in case of excessive memory pressure (i.e. long GCs/OOM).
<code>tpc_pending_requests_limit</code> - the number of pending read/write requests per core, to be increased only if getting too many timeouts. </p>
<p>Try raising this and repeat your tests, continue to monitor GC and CPU utilization.</p>
<h3>upgrading version!</h3>
<p>DSE 6.8 adds work stealing, and badly balanced data models, or apps with anti-patterns have a less significant impact on the cluster health. The downside to this is
writes will still go to a particular Core and this means data models </p>
<h3>raising work stealing</h3>
<p>By default in DSE 6.8 work stealing is pretty conservative as the developers wanted us to get the benefit of TPC, but nearly every cluster does seem to have an application
that would benefit from work stealing so it is there. However, some clusters contain apps that primarily have anti-patterns, and in these cases the defaults for work stealing maybe too minimal.</p>
<h4>track work stealing and pending cores</h4>
<ul>
<li>using something like <code>sperf core statuslogger</code> try and see if there is a high number of pending operations, especially on singular cores, if so you may benefit from raising work stealing</li>
<li>track how much work stealing is going on, if it is consistently happening, that is a hint there is a balance issue and raising it more may help, <a href="https://github.com/foundev/notebooks/blob/main/performance/analysis-general.ipynb">my notebook on performance</a> can graph
this for you, the key thing you are looking for is <em>sustained</em> work stealing.</li>
</ul>
<p>Anothing thing you can use is <a href="https://github.com/BurntSushi/ripgrep#installation">ripgrep</a> in your debug.log with this regex <code>DEBUG \[(?P&lt;thread&gt;.*)\] (?P&lt;date&gt;.{10} .{12}) *(?P&lt;source_file&gt;[^:]*):(?P&lt;source_line&gt;[0-9]*) - Stole (?P&lt;tasks&gt;\d+) tasks since last time\.</code></p>
<h5>Actions</h5>
<p>to the jmv-server.options add the following and reboot:</p>
<p><code>-Ddse.tpc.work_stealing_max_unparks=&lt;half of tpc_cores&gt; (defaults to 1/4 of the TPC cores so if I had 8 cores, the default is 2, raising that to 4 may help work stealing) 
-Ddse.tpc.work_stealing_max_tasks=512 (default = 32)</code></p>
<p>Review if this helps the total pending tasks or not.</p>
<h3>changing data model</h3>
<p>Writes will continue to pin to an individual core. The classic very naive time series data model where the partition key is a time bucket, will in the case of TPC pin
all writes to one core, and all requests to that core will be backed up. Eventually this will lead to backpressure, despite the node itself not being very much
stress and in fact being very underused.</p>
<h4>Example TPC stats output for this sort of situation</h4>
<p>Scenario:</p>
<ul>
<li>time is bucketed on a per minute basis and this is the partition key for all writes</li>
<li>there are 4 cores per node</li>
<li>there are 6 nodes with RF 3</li>
</ul>
<h5>At 2021-07-28 11:00:00 the write traffic for the next minute can looks like this</h5>
<p><em>cores list total pending writes</em> </p>
<p><code>| node | cpu usage | cores1    | core2 | core3 | core4 | 
| ---- | --------- | –––------ | --–-— | ----- | ----- | 
| 1    | 25%       | 134211    | 0     | 0     |  0    |
| 2    | 25%       | 0         | 0     | 0     | 12441 |
| 3    | 25%       | 0         | 13441 | 0     | 0     |
| 4    | 0%        | 0         | 0     | 0     | 0     |
| 5    | 0%        | 0         | 0     | 0     | 0     |
| 6    | 0%        | 0         | 0     | 0     | 0     |</code></p>
<h5>At 2021-07-28 11:01:00 the write traffic for the next minute can looks like this</h5>
<p><code>| node | cpu usage | cores1    | core2 | core3 | core4 | 
| ---- | --------- | –––------ | --–-— | ----- | ----- | 
| 1    | 0%        | 0         | 0     | 0     | 0     |
| 2    | 25%       | 0         | 0     | 0     | 14552 |
| 3    | 25%       | 0         | 12442 | 0     | 0     |
| 4    | 25%       | 0         | 0     | 12498 | 0     |
| 5    | 0%        | 0         | 0     | 0     | 0     |
| 6    | 0%        | 0         | 0     | 0     | 0     |</code></p>
<p>You get the idea, this repeats like this for every minute, using effectivley only one CPU core on 3 nodes for an entire minute which is not something useful from a scaling standpoint an exposes problematic data models and hotspots pretty quickly, worse it will have global effects if those individual
cores hit the backpressure threshold levels.</p>
<h4>The fix</h4>
<p>The only fix for this is to change the data model and add in an extra column to the partition key which will require a new table. There are a few common approaches:</p>
<ul>
<li>add a known value from the data that is not so time deterministic, and still well enough distributed.  For example, if there is a state (something well known) you can use that and just query for all states during reads.</li>
<li>add a known value that is known ahead of time, something like [1-30] and randomly pick one on each write, clients need to know to query all of those extra buckets.</li>
<li>add a random value but track that the random value is in that time bucket so you can go back and query those. This has a lot of quirks and can be tricky, I do not see this very much now.</li>
</ul>
<h2>Conclusion</h2>
<p>This was a rather simplistic view of backpressure, and those that have moved beyond this level of depth can safely ignore this article, it is intended to make an audience understand the most basic behaviors of TPC Backpressure 
and is not to be viewed as the final word on backpressure tuning. This will help you out with 80 % of the cases where backpressure is enabled and what are the next steps.</p>