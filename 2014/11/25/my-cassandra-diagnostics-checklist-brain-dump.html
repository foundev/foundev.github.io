<hr />
<p>wordpress_id: 171
title: My Cassandra 2.0 Diagnostics Checklist (Brain Dump)
date: 2014-11-25T21:33:44+00:00
author: Ryan Svihla
layout: post
wordpress_guid: http://lostechies.com/ryansvihla/?p=171
dsq_thread_id:
  - "3263339991"
categories:
  - Cassandra
tags:
  - Cassandra</p>
<hr />
<p>UPDATE:</p>
<p>This list needs to be updated and as of today only has been verified with Cassandra 2.0.</p>
<p>Original Blog Post:</p>
<p>This isn&#8217;t remotely complete, but I had a colleague ask me to do a brain dump of my process and this is by and large usually it. I&#8217;m sure this will leave more questions than answers for many of you, and I&#8217;d like to follow up this post at some point with detail of the why and the how of a lot of it so it can be more useful to beginners. Today this is in a very raw form.</p>
<h1>Important metrics to collect</h1>
<ul>
<li>logs</li>
<li>cassandra.yaml</li>
<li>cassandra-env.sh</li>
<li>histograms</li>
<li>tpstats</li>
<li>schema of all tables</li>
<li>nodetool status</li>
<li>ulimit -a as user that cassandra is running as (make sure it matches our settings)</li>
</ul>
<h1>Import facts to collect</h1>
<ul>
<li>heap usage under load, is it hitting 3/4 of MAX_HEAP ?</li>
<li>Pending compactions (opscenter, will have to look up JMX metric later)</li>
<li>size of writes/reads</li>
<li>max partition size (histograms will say)</li>
<li>tps per node</li>
<li>list of queries run against the cluster</li>
</ul>
<h1>Bad things to look for in logs</h1>
<ul>
<li>ERROR</li>
<li>WARN</li>
<li>dropped</li>
<li>GCInspector (see parnews over 200ms and CMS )</li>
<li>Emergency</li>
<li>Out Of Memory</li>
</ul>
<h1>Typically screwed up cassandra-env.sh settings</h1>
<ul>
<li>heap not set to 8gb</li>
<li>parnew no more than 800mb (unless using tunings from https://issues.apache.org/jira/browse/CASSANDRA-8150)</li>
</ul>
<h1>Typically screwed up cassandra.yaml settings</h1>
<ul>
<li>row cache being enabled (unless 95% read with even width rows)</li>
<li>vnodes set with solr</li>
<li>system_auth keyspace still with RF 1 and SimpleStrategy</li>
<li>flush_writers set to crazy high level (varies by disk configuration, follow documentation advice, double digits is suspect)</li>
<li>rcp_address: 0.0.0.0 (slows down certain versions of the driver)</li>
<li>multithreaded_compaction: true (almost always wrong)</li>
</ul>
<h1>Typically bad things in histograms</h1>
<ul>
<li>double hump</li>
<li>long long tail</li>
<li>partitions with cell count over 100k</li>
<li>partitions with size over in_memory_compaction_limit (default 64mb)</li>
</ul>
<h1>Typically bad things in tpstats</h1>
<ul>
<li>dropped (anything) especially mutations.</li>
<li>blocked flush writers (if all time is in the 100s it&#8217;s usually a problem)</li>
</ul>
<h1>Typically screwed up keyspace settings</h1>
<ul>
<li>STC compaction in use on SSD when customers have a low read SLA (or no defined one).</li>
<li>Using RF less than 3 per DC</li>
<li>Using RF more than 3 per DC</li>
<li>Using SimpleStrategy with multiDC</li>
</ul>
<h1>Typically screwed up CF settings</h1>
<ul>
<li>read_repair_chance and dclocal_read_repair_chance adding up to more than 0.1 is usually a bad tradeoff.</li>
<li>Secondary indexes in use (on writes think write amplification, and on reads think synchronous full cluster scan).</li>
<li>Is system_auth replicated correctly? And has it has repair run after this was changed? If you see auth errors in the log..the answer is probably no</li>
</ul>
<h1>Typically bad things in nodetool status</h1>
<ul>
<li>use of racks is not even (4 in one rack and 2 in another..that&#8217;s a no no)</li>
<li>use of racks is not enough to fulfil muliple of RF (if you have 2 racks of 2 and RF 3..how will that get evenly laid out?).</li>
<li>if load is wildly off. may not mean anything, but go look on disk, if the cassandra data files are really imbalanced badly figure out why.</li>
</ul>
<h1>Performance tuning Cassandra for write heavy workloads</h1>
<ul>
<li>Run cstress as a baseline</li>
<li>Run cstress with appications write size this will often identify bottlenecks</li>
<li>Do math on writes and desired TPS. Are the writes saturating the network? Don&#8217;t forget bits and bytes are different ðŸ™‚</li>
<li>Lower ParNew for lower <em>peak</em> 99th, now this flies in the face of what is happening with this Jira (https://issues.apache.org/jira/browse/CASSANDRA-8150), but until I&#8217;ve worked through all of that ParNew lower than 800MB is generally a good way to tradeoff throughput for smaller ParNew GCs.</li>
<li>Run Fio with the following profile https://gist.github.com/tobert/10685735 (adjusted to match users system). Using these as baseline (http://tobert.org/disk-latency-graphs/). </li>
</ul>
<h1>Performance Tuning Code for write heavy workloads</h1>
<p>Once you&#8217;ve established the system is awesome. Review queries and code.</p>
<ul>
<li>Things to look for BATCH keyword for bulk loading (fine for consistency, but has to take into account SLA hit if the writes are larger than BATCH can handle).</li>
<li>If using batches what is the write size?</li>
<li>Are you using a thrift driver and destroying one or two nodes because of bad load balancing?</li>
<li>Are you using the DataStax driver, is it using the token aware policy (shuffle on with 2.0.8 ideally)?</li>
<li>If using the DataStax driver, is it the latest 2.0.x Lots of useful fixes in each release, it really matters.</li>
<li>Using LWT? They involve 4 round trips and so ..while they&#8217;re awesome they&#8217;re slower.</li>
</ul>